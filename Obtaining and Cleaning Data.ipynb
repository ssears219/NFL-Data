{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining and Cleaning Data from Flat File, HTML, and API\n",
    "\n",
    "### Resulting 3 data frames saved in Excel files\n",
    " - NFL Games Data.xlsx\n",
    " - NFL Season Stats Data.xlsx\n",
    " - NFL Game Weather Data.xlsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flat File:** nfl_games.csv (https://www.kaggle.com/toddwfloyd/footballscores/version/1#) \n",
    "**Description:** Tabular data representing each NFL game from 1920 to 2017.  \n",
    "**Variables:**\n",
    "1.\tdate (date)\n",
    "2.\tseason (integer) - year\n",
    "3.\tneutral (binary) – denotes whether it was a neutral site\n",
    "4.\tplayoff (binary) – denotes whether it was a playoff game\n",
    "5.\tteam1 (string) – denotes home team\n",
    "6.\tteam2 (string) – denotes away team\n",
    "7.\telo1 (float) – rating for home team\n",
    "8.\telo2 (float) – rating for away team\n",
    "9.\tscore1 (integer) – score for home team\n",
    "10.\tscore2 (integer) – score for away team\n",
    "11.\tresult1 (float) – denotes whether home team won, 0.5 denotes tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website Data:** https://www.pro-football-reference.com/years/  \n",
    "**Description:** Adding the year on to the end of this url leads you to a page holding season statistics for each NFL season from 1920 to 2019, but I will go to 2017 (not including the 2017 season).  \n",
    "**Variables to Scrape:**\n",
    "1. year - season\n",
    "2. team\n",
    "3. wins\n",
    "4. losses\n",
    "5. ties\n",
    "6. win loss perc - win loss percentage\n",
    "7. points - points scored\n",
    "8. points opp - points allowed\n",
    "9. points diff - point differential\n",
    "10. mov - margin of victory, (point differential / games played)\n",
    "11. sos total - strength of schedule using Simple Rating System (SRS)\n",
    "12. srs total - Team quality relative to average based on Simple Rating System, (srs = mov + sos = osrs + dsrs)\n",
    "13. srs offense - Offense relative to average (osrs)\n",
    "14. srs defense - Defense relative to average (dsrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Data:** https://rapidapi.com/awigmore/api/visual-crossing-weather  \n",
    "**Description:** This api is able to return historical weather data for a specified period of time and City.  \n",
    "**Variables:**  \n",
    "1.\tCity\n",
    "2.\tDate\n",
    "3.\tMin Temperature\n",
    "4.\tMax Temperature\n",
    "5.\tWind Speed\n",
    "6.\tWind Gust\n",
    "7.\tWind Direction\n",
    "8.\tPrecipitation\n",
    "9.\tPrecipitation Cover\n",
    "10.\tSnow Depth\n",
    "11.\tVisibility\n",
    "12.\tWeather Type  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from plotnine import *\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat File Data\n",
    "- Update games headers to indicate home/away\n",
    "- Add team name and location data  \n",
    "    a. Load additional team data CSV file for mapping  \n",
    "    b. Clean up team data CSV file in preparation for merge  \n",
    "    c. Merge with games  \n",
    "- Update winner column to have team values instead of binary and add a loser column\n",
    "- Rearrange columns\n",
    "- Update neutral column to be game location\n",
    "- Update Locations where only States or team names are present\n",
    "- Add stadium type column for teams with indoor stadiums\n",
    "- Add location updates by years for Rams and Chargers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\ssear\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "games = pd.read_csv(\"nfl_games.csv\")\n",
    "games.rename(columns={'team1':'home', 'team2': 'away', 'elo1': 'home elo', 'elo2': 'away elo',\n",
    "                      'elo_prob1': 'home elo prob', 'score1': 'home score', 'score2': 'away score',\n",
    "                      'result1': 'winner'}, inplace = True)\n",
    "\n",
    "# Update games headers to indicate home/away\n",
    "list_of_teams = games['home']\n",
    "list_of_teams.append(games['away'])\n",
    "\n",
    "# Add team name and location data\n",
    "teams = pd.read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/teams.csv\")\n",
    "teams.drop(['season', 'pff', 'nfl', 'pfr', 'pfflabel', 'fo', 'short_location', 'hyphenated'], axis = 1, inplace=True)\n",
    "teams.drop_duplicates(subset='team', inplace=True, keep='first')\n",
    "teams.rename(columns={'full':'home full', 'location': 'home location', 'nickname': 'home nickname'}, inplace=True)\n",
    "\n",
    "games = pd.merge(games, teams, left_on='home', right_on='team',how='left')\n",
    "games.drop(['team'], axis=1, inplace=True)\n",
    "\n",
    "teams.rename(columns={'home full':'away full', 'home location': 'away location',\n",
    "                      'home nickname': 'away nickname'}, inplace=True)\n",
    "\n",
    "games = pd.merge(games, teams, left_on='away', right_on='team',how='left')\n",
    "games.drop(['team'], axis=1, inplace=True)\n",
    "\n",
    "# Update winner column to have team values instead of binary and add a loser column\n",
    "games['loser'] = games['winner'] # creates a new column for loser with placeholder winner values for now\n",
    "\n",
    "for i in range(len(games['winner'])):\n",
    "    val = games['winner'][i]\n",
    "    if val == 1:\n",
    "        games['winner'][i] = games['home'][i]\n",
    "        games['loser'][i] = games['away'][i]\n",
    "    elif val == 0:\n",
    "        games['winner'][i] = games['away'][i]\n",
    "        games['loser'][i] = games['home'][i]\n",
    "    else:\n",
    "        games['winner'][i] = 'NaN'\n",
    "        games['loser'][i] = 'NaN'\n",
    "\n",
    "# Rearrange columns\n",
    "games = games[['date', 'season', 'neutral', 'playoff', 'home', 'away', 'home elo',\n",
    "       'away elo', 'home elo prob', 'home score', 'away score', 'winner', 'loser',\n",
    "       'home full', 'away full', 'home location', 'away location', 'home nickname', \n",
    "        'away nickname']]\n",
    "\n",
    "# Update neutral column to be game location\n",
    "for i in range(len(games['neutral'])):\n",
    "    val = games['neutral'][i]\n",
    "    if val == 1:\n",
    "        games['neutral'][i] = 'neutral'\n",
    "    elif val == 0:\n",
    "        games['neutral'][i] = games['home location'][i]\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "games = games.rename(columns={'neutral':'game location'})\n",
    "\n",
    "# Update Locations where only States or team names are present\n",
    "games['game location'] = games['game location'].replace(['New York Giants', 'New York Jets'], 'East Rutherford')\n",
    "games['home location'] = games['home location'].replace(['New York Giants', 'New York Jets'], 'East Rutherford')\n",
    "games['away location'] = games['away location'].replace(['New York Giants', 'New York Jets'], 'East Rutherford')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Minnesota', 'Minneapolis')\n",
    "games['home location'] = games['home location'].replace('Minnesota', 'Minneapolis')\n",
    "games['away location'] = games['away location'].replace('Minnesota', 'Minneapolis')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Arizona', 'Glendale Arizona')\n",
    "games['home location'] = games['home location'].replace('Arizona', 'Glendale Arizona')\n",
    "games['away location'] = games['away location'].replace('Arizona', 'Glendale Arizona')\n",
    "\n",
    "games['game location'] = games['game location'].replace('New England', 'Foxborough Massachusetts')\n",
    "games['home location'] = games['home location'].replace('New England', 'Foxborough Massachusetts')\n",
    "games['away location'] = games['away location'].replace('New England', 'Foxborough Massachusetts')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Tennessee', 'Nashville Tennessee')\n",
    "games['home location'] = games['home location'].replace('Tennessee', 'Nashville Tennessee')\n",
    "games['away location'] = games['away location'].replace('Tennessee', 'Nashville Tennessee')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Carolina', 'Charlotte North Carolina')\n",
    "games['home location'] = games['home location'].replace('Carolina', 'Charlotte North Carolina')\n",
    "games['away location'] = games['away location'].replace('Carolina', 'Charlotte North Carolina')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Tampa Bay', 'Tampa Florida')\n",
    "games['home location'] = games['home location'].replace('Tampa Bay', 'Tampa Florida')\n",
    "games['away location'] = games['away location'].replace('Tampa Bay', 'Tampa Florida')\n",
    "\n",
    "games['game location'] = games['game location'].replace('Los Angeles Chargers', 'Los Angeles')\n",
    "games['home location'] = games['home location'].replace('Los Angeles Chargers', 'Los Angeles')\n",
    "games['away location'] = games['away location'].replace('Los Angeles Chargers', 'Los Angeles')\n",
    "\n",
    "# Add stadium type column for teams with indoor stadiums\n",
    "games['stadium type'] = games['game location']\n",
    "for i in range(len(games['game location'])):\n",
    "    val = games['game location'][i]\n",
    "    if val in ['Detroit', 'New Orleans', 'Minneapolis', 'Dallas', 'Indianapolis',\n",
    "               'Atlanta', 'Houston', 'Glendale Arizona']:\n",
    "        games['stadium type'][i] = 'indoor'\n",
    "    elif type(val) is float:\n",
    "        games['stadium type'][i] = 'NaN'\n",
    "    else:\n",
    "        games['stadium type'][i] = 'outdoor'\n",
    "        \n",
    "# Add location updates by years for Rams and Chargers\n",
    "for i in range(len(games['season'])):\n",
    "    \n",
    "    if games['home'][i] == 'LAR':\n",
    "        if games['season'][i] >= 1936 and games['season'][i] <= 1945:\n",
    "            games['game location'][i] = 'Cleveland Ohio'\n",
    "            games['home location'][i] = 'Cleveland Ohio'\n",
    "        elif games['season'][i] >= 1946 and games['season'][i] < 1995:\n",
    "            games['game location'][i] = 'Los Angeles'\n",
    "            games['home location'][i] = 'Los Angeles'\n",
    "        elif games['season'][i] >= 1996 and games['season'][i] <= 2015:\n",
    "            games['game location'][i] = 'Saint Louis Missouri'\n",
    "            games['home location'][i] = 'Saint Louis Missouri'\n",
    "        elif games['season'][i] > 2015 and games['season'][i] <= 2020:\n",
    "            games['game location'][i] = 'Los Angeles'\n",
    "            games['home location'][i] = 'Los Angeles'\n",
    "            \n",
    "    if games['away'][i] == 'LAR':\n",
    "        if games['season'][i] >= 1936 and games['season'][i] <= 1945:\n",
    "            games['away location'][i] = 'Cleveland Ohio'\n",
    "        elif games['season'][i] >= 1946 and games['season'][i] < 1995:\n",
    "            games['away location'][i] = 'Los Angeles'\n",
    "        elif games['season'][i] >= 1996 and games['season'][i] <= 2015:\n",
    "            games['away location'][i] = 'Saint Louis Missouri'\n",
    "        elif games['season'][i] > 2015 and games['season'][i] <= 2020:\n",
    "            games['away location'][i] = 'Los Angeles'\n",
    "            \n",
    "    if games['home'][i] == 'LAC':\n",
    "        if games['season'][i] == 1960:\n",
    "            games['game location'][i] = 'Los Angeles'\n",
    "            games['home location'][i] = 'Los Angeles'\n",
    "        elif games['season'][i] > 1960 and games['season'][i] < 2017:\n",
    "            games['game location'][i] = 'San Diego'\n",
    "            games['home location'][i] = 'San Diego'\n",
    "        elif games['season'][i] >= 2017:\n",
    "            games['game location'][i] = 'Los Angeles'\n",
    "            games['home location'][i] = 'Los Angeles'\n",
    "            \n",
    "    if games['away'][i] == 'LAC':\n",
    "        if games['season'][i] == 1960:\n",
    "            games['away location'][i] = 'Los Angeles'\n",
    "        elif games['season'][i] > 1960 and games['season'][i] < 2017:\n",
    "            games['away location'][i] = 'San Diego'\n",
    "        elif games['season'][i] >= 2017:\n",
    "            games['away location'][i] = 'Los Angeles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframe to Excel so I don't need to re-run scripts\n",
    "games.to_excel('NFL Games Data.xlsx', sheet_name='Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = games[\"season\"].unique()\n",
    "\n",
    "# Create Dataframe to store captured data\n",
    "web_data = pd.DataFrame(columns = ['year','team','wins','losses','ties','win loss perc','points',\n",
    "                                   'points opp','points diff','mov','sos total','srs total',\n",
    "                                   'srs offense','srs defense'])\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    # Get html from url for year\n",
    "    my_url = \"https://www.pro-football-reference.com/years/{}/\".format(year)\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "    \n",
    "    # Get tables to extract data from\n",
    "    afc_nfc_tables = page_soup.findAll(\"table\", {\"class\":\"sortable stats_table\"})\n",
    "\n",
    "    # Parse conference table(s)\n",
    "    for conf in range(len(afc_nfc_tables)):\n",
    "        conference_table_rows = afc_nfc_tables[conf].tbody.findAll(\"tr\")\n",
    "\n",
    "        # Parse row of conference table\n",
    "        for row in range(len(conference_table_rows)):\n",
    "\n",
    "            # 'right left' row denotes the NFL Division and must be skipped\n",
    "            if len(conference_table_rows[row].findAll(\"td\", {\"class\":\"right left\"})) == 0:\n",
    "\n",
    "                # Capture data for one row\n",
    "                team_name = conference_table_rows[row].a.text\n",
    "                wins = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"wins\"})[0].text\n",
    "                losses = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"losses\"})[0].text\n",
    "\n",
    "                # One year did not have ties\n",
    "                if len(conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"ties\"})) == 0:\n",
    "                    ties = 0\n",
    "                else:\n",
    "                    ties = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"ties\"})[0].text\n",
    "                \n",
    "                # Capture data for one row (continued)\n",
    "                win_loss_perc = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"win_loss_perc\"})[0].text\n",
    "                points = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"points\"})[0].text\n",
    "                points_opp = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"points_opp\"})[0].text\n",
    "                points_diff = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"points_diff\"})[0].text\n",
    "                mov = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"mov\"})[0].text\n",
    "                sos_total = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"sos_total\"})[0].text\n",
    "                srs_total = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"srs_total\"})[0].text\n",
    "                srs_offense = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"srs_offense\"})[0].text\n",
    "                srs_defense = conference_table_rows[row].findAll(\"td\", {\"data-stat\":\"srs_defense\"})[0].text\n",
    "\n",
    "\n",
    "                # Add captured row to dataframe\n",
    "                new_row = {'year':year, 'team':team_name, 'wins':wins, 'losses':losses, 'ties':ties,\n",
    "                           'win loss perc':win_loss_perc, 'points':points, 'points opp':points_opp,\n",
    "                           'points diff':points_diff, 'mov': mov, 'sos total': sos_total, 'srs total':srs_total,\n",
    "                           'srs offense': srs_offense, 'srs defense':srs_defense}\n",
    "                web_data = web_data.append(new_row, ignore_index=True)\n",
    "                \n",
    "# Convert number columns to numeric\n",
    "cols = ['year', 'wins','losses','ties', 'win loss perc','points', 'points opp','points diff',\n",
    "         'mov','sos total','srs total', 'srs offense','srs defense']\n",
    "\n",
    "web_data[cols] = web_data[cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframe to Excel so I don't need to re-run scripts\n",
    "web_data.to_excel('NFL Season Stats Data.xlsx', sheet_name='Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920-09-26 2017-02-05\n"
     ]
    }
   ],
   "source": [
    "# Get array of game locations to iterate through\n",
    "game_locations = games['game location'].unique()\n",
    "\n",
    "# Get start and end dates and convert them into correct format for API\n",
    "end_date = games['date'][len(games['date'])-1]\n",
    "end_date = str(datetime.strptime(end_date, '%m/%d/%Y').date())\n",
    "start_date = games['date'][0]\n",
    "start_date = str(datetime.strptime(start_date, '%m/%d/%Y').date())\n",
    "print(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year intervals to meet max row requirements of API\n",
    "intervals = [(1961,1980),(1981,2000),(2001,2017)]\n",
    "\n",
    "# Empty list of dataframes to concat later\n",
    "df_list = []\n",
    "\n",
    "# Response counter\n",
    "resp_cnt = 0\n",
    "\n",
    "# Loop through game locations, pulling weather data back for each location\n",
    "for location in game_locations:\n",
    "    for interval in intervals:\n",
    "        start = str(interval[0])\n",
    "        end = str(interval[1])\n",
    "        if type(location) is not float and location != 'neutral':\n",
    "            url = \"https://visual-crossing-weather.p.rapidapi.com/history\"\n",
    "\n",
    "            querystring = {\"dayStartTime\":\"12:00:00\",\"contentType\":\"csv\",\"dayEndTime\":\"23:00:00\",\n",
    "                           \"shortColumnNames\":\"false\",\"startDateTime\":start+\"-01-01T00:00:00\",\n",
    "                           \"aggregateHours\":\"24\",\"location\":location+\" USA\",\n",
    "                           \"endDateTime\":end+\"-12-31T00:00:00\",\"unitGroup\":\"us\"}\n",
    "            \n",
    "            # Creds removed\n",
    "            headers = {\n",
    "                'x-rapidapi-host': \"\", \n",
    "                'x-rapidapi-key': \"\"\n",
    "                }\n",
    "            \n",
    "            response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "            resp_cnt = resp_cnt + 1\n",
    "            temp_df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "            df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes into one\n",
    "weather_data =  pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chicago, IL, United States', 'Glendale, AZ, United States',\n",
       "       'Green Bay, WI, United States',\n",
       "       'East Rutherford, NJ, United States', 'Detroit, MI, United States',\n",
       "       'Washington, DC, United States', 'Pittsburgh, PA, United States',\n",
       "       'Philadelphia, PA, United States', 'Cleveland, OH, United States',\n",
       "       'San Francisco, CA, United States',\n",
       "       'Los Angeles, CA, United States',\n",
       "       'Indianapolis, IN, United States', 'Foxborough, MA, United States',\n",
       "       'Oakland, CA, United States', 'Nashville, TN, United States',\n",
       "       'Buffalo, NY, United States', 'Dallas, TX, United States',\n",
       "       'Kansas City, MO, United States', 'Denver, CO, United States',\n",
       "       'Minneapolis, MN, United States', 'San Diego, CA, United States',\n",
       "       'Miami, FL, United States', 'Atlanta, GA, United States',\n",
       "       'New Orleans, LA, United States', 'Cincinnati, OH, United States',\n",
       "       'Seattle, WA, United States', 'Tampa, FL, United States',\n",
       "       'Jacksonville, FL, United States', 'Charlotte, NC, United States',\n",
       "       'Baltimore, MD, United States', 'St Louis, MO, United States',\n",
       "       'Houston, TX, United States'], dtype=object)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if Addresses were correct\n",
    "weather_data['Resolved Address'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting dates in the same format\n",
    "def convert_date(date):\n",
    "    return str(datetime.strptime(date, '%m/%d/%Y').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dates with function\n",
    "weather_data['Date time'] = weather_data['Date time'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1961-01-01\n",
       "1         1961-01-02\n",
       "2         1961-01-03\n",
       "3         1961-01-04\n",
       "4         1961-01-05\n",
       "             ...    \n",
       "687022    2017-12-27\n",
       "687023    2017-12-28\n",
       "687024    2017-12-29\n",
       "687025    2017-12-30\n",
       "687026    2017-12-31\n",
       "Name: Date time, Length: 687027, dtype: object"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data['Date time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77880, 25)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows in weather data where for dates there's no game\n",
    "weather_data = weather_data[weather_data['Date time'].isin(loc_dates['date'])]\n",
    "weather_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the ' USA' part of the location in the weather data df\n",
    "weather_data['Address'].replace(regex=[' USA'], value = '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataframe to Excel so I don't need to re-run scripts\n",
    "weather_data.to_excel('NFL Game Weather Data.xlsx', sheet_name='Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
